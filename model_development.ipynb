{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "\n",
    "from lightning.pytorch import seed_everything, Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "from core.config import (\n",
    "    BATCH_SIZE,\n",
    "    LR,\n",
    "    MAX_DURATION,\n",
    "    N_EPOCHS,\n",
    "    RANDOM_STATE,\n",
    "    SAMPLE_RATE,\n",
    "    WEIGHT_DECAY,\n",
    ")\n",
    "from core.data import load_data\n",
    "from core.downstream import LightningMLP\n",
    "from core.transforms import load_transforms\n",
    "from core.upstream import load_feature_extractor, load_processor\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script\n",
    "\n",
    "We will dissect the ```core.train``` script using the tiny Whisper model as an example\n",
    "\n",
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "\tdataset: nort3160\n",
      "\tmodel_id: whisper_tiny\n",
      "\tdtype: speech\n",
      "\tmax_duration: 30.0\n",
      "\ttransform: True\n",
      "\tdevice: cuda:0\n",
      "\tlr: 0.00025\n",
      "\tweight_decay: 0.001\n",
      "\tbatch_size: 64\n",
      "\tn_epochs: 3\n",
      "\tdata_dir: data\n",
      "\tcache_dir: /home/common/speech_phylo/models\n",
      "\tseed: 42\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(\n",
    "    # Name of the dataset\n",
    "    dataset = \"nort3160\",\n",
    "    # Name of the backbone model\n",
    "    model_id = \"whisper_tiny\",\n",
    "    # Data type\n",
    "    dtype = \"speech\",\n",
    "    # Max audio duration\n",
    "    max_duration = MAX_DURATION,\n",
    "    # Whether to transform the data using Voice Activity Detection or not\n",
    "    transform = True,\n",
    "    # Device on which a torch.Tensor is or will be allocated\n",
    "    device = \"cuda:0\",\n",
    "    # Learning rate\n",
    "    lr = LR,\n",
    "    # Weight decay\n",
    "    weight_decay = WEIGHT_DECAY,\n",
    "    # Batch size\n",
    "    batch_size = BATCH_SIZE,\n",
    "    # Number of training epochs\n",
    "    n_epochs = N_EPOCHS,\n",
    "    # Path to main data folder path\n",
    "    data_dir = \"data\", # Change that folder if needed!\n",
    "    # Path where cached models are stored\n",
    "    cache_dir = \"/home/common/speech_phylo/models\",  # Change that folder if needed!\n",
    "    # Random seed\n",
    "    seed = RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in vars(args).items():\n",
    "    print(f\"\\t{k}: {v}\")\n",
    "\n",
    "if not (torch.cuda.is_available() and \"cuda\" in args.device):\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(args.seed, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature extractor...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "WhisperFeatureExtractor                            [1, 1, 384]               --\n",
       "├─Whisper: 1-1                                     --                        --\n",
       "│    └─AudioEncoder: 2-1                           --                        --\n",
       "│    │    └─Conv1d: 3-1                            [1, 384, 3000]            92,544\n",
       "│    │    └─Conv1d: 3-2                            [1, 384, 1500]            442,752\n",
       "│    │    └─ModuleList: 3-3                        --                        7,096,320\n",
       "│    │    └─LayerNorm: 3-4                         [1, 1500, 384]            768\n",
       "│    └─TextDecoder: 2-2                            --                        172,032\n",
       "│    │    └─Embedding: 3-5                         [1, 1, 384]               19,916,160\n",
       "│    │    └─ModuleList: 3-6                        --                        9,463,296\n",
       "│    │    └─LayerNorm: 3-7                         [1, 1, 384]               768\n",
       "====================================================================================================\n",
       "Total params: 37,184,640\n",
       "Trainable params: 37,184,640\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 978.24\n",
       "====================================================================================================\n",
       "Input size (MB): 0.06\n",
       "Forward/backward pass size (MB): 258.23\n",
       "Params size (MB): 148.05\n",
       "Estimated Total Size (MB): 406.34\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading feature extractor...\")\n",
    "feature_extractor = load_feature_extractor(\n",
    "    model_id=args.model_id,\n",
    "    cache_dir=args.cache_dir,\n",
    "    device=args.device,\n",
    ")\n",
    "\n",
    "summary(feature_extractor, input_size=(1, SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Loading processor...\n",
      "Dataset args:\n",
      "{'data_dir': 'data',\n",
      " 'dataset': 'nort3160',\n",
      " 'max_duration': 30.0,\n",
      " 'processor': <core.upstream.processing.WhisperProcessor object at 0x7f9f4f42a140>,\n",
      " 'transform': Sequential(\n",
      "  (0): Vad()\n",
      "  (1): Trim()\n",
      "  (2): Pad()\n",
      ")}\n",
      "Data loader args:\n",
      "{'batch_size': 64, 'num_workers': 4, 'pin_memory': True}\n",
      "Loading data..\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing data...\")\n",
    "dataset_args = {\n",
    "    \"dataset\": args.dataset,\n",
    "    \"data_dir\": args.data_dir,\n",
    "}\n",
    "\n",
    "loader_args = {\n",
    "    \"num_workers\": 4,\n",
    "    \"batch_size\": args.batch_size,\n",
    "    \"pin_memory\": True,\n",
    "}\n",
    "\n",
    "if args.dtype == \"speech\":\n",
    "    print(\"Loading processor...\")\n",
    "    processor = load_processor(\n",
    "        model_id=args.model_id,\n",
    "        sr=SAMPLE_RATE,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "\n",
    "    transform = load_transforms(\n",
    "        sr=SAMPLE_RATE, max_duration=args.max_duration, vad=args.transform\n",
    "    )\n",
    "\n",
    "    dataset_args = {\n",
    "        **dataset_args,\n",
    "        \"processor\": processor,\n",
    "        \"max_duration\": args.max_duration,\n",
    "        \"transform\": transform,\n",
    "    }\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "print(\"Dataset args:\")\n",
    "pprint(dataset_args)\n",
    "print(\"Data loader args:\")\n",
    "pprint(loader_args)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_dataset, valid_dataset, test_dataset = load_data(**dataset_args)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, **loader_args)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, **loader_args)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, **loader_args)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing downstream classifier...\n",
      "MLP(\n",
      "  (classifier): Linear(in_features=384, out_features=5, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing downstream classifier...\")\n",
    "lit_mlp = LightningMLP(\n",
    "    feature_extractor=feature_extractor,\n",
    "    num_classes=len(train_dataset.label_encoder),\n",
    "    loss_fn=nn.NLLLoss(),\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "torch.compile(lit_mlp)\n",
    "\n",
    "print(lit_mlp.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mneclow\u001b[0m (\u001b[33mphylo2vec\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>data/eval/wandb/run-20241027_153234-y8lahp4m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/phylo2vec/mlops_project_eval_nort3160/runs/y8lahp4m' target=\"_blank\">snowy-blaze-11</a></strong> to <a href='https://wandb.ai/phylo2vec/mlops_project_eval_nort3160' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/phylo2vec/mlops_project_eval_nort3160' target=\"_blank\">https://wandb.ai/phylo2vec/mlops_project_eval_nort3160</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/phylo2vec/mlops_project_eval_nort3160/runs/y8lahp4m' target=\"_blank\">https://wandb.ai/phylo2vec/mlops_project_eval_nort3160/runs/y8lahp4m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dir = f\"{args.data_dir}/eval\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "wandb_logger = WandbLogger(\n",
    "    project=f\"mlops_project_eval_{args.dataset}\", save_dir=eval_dir\n",
    ")\n",
    "\n",
    "wandb_logger.experiment.config.update(\n",
    "    {\n",
    "        \"model_id\": args.model_id,\n",
    "        \"max_duration\": args.max_duration,\n",
    "        \"transform\": args.transform,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"cuda\" in args.device:\n",
    "    accelerator = \"gpu\"\n",
    "    devices = [int(args.device.split(\":\")[-1])]\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    devices = \"auto\"\n",
    "\n",
    "print(\"Start training!\")\n",
    "trainer = Trainer(\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    "    max_epochs=args.n_epochs,\n",
    "    enable_model_summary=True,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor=\"valid_loss\", mode=\"min\", save_last=True),\n",
    "        TQDMProgressBar(),\n",
    "    ],\n",
    "    logger=wandb_logger,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=lit_mlp,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=valid_loader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
